{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VEkgaJCESbv",
    "outputId": "4f5bdbcf-7ee2-48d3-ec26-b8a8acd37408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.14.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.22.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.7.1)\n",
      "Requirement already satisfied: onnxruntime-gpu in /usr/local/lib/python3.10/dist-packages (1.15.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (23.5.26)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.22.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (23.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (3.20.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.11.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.6)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx\n",
    "!pip install onnxruntime-gpu\n",
    "!pip install transformers\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8Igus6RREaD4"
   },
   "outputs": [],
   "source": [
    "# # Load pretrained model and tokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    temp =  torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return F.normalize(temp, p=2, dim=1)\n",
    "\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True, )\n",
    "model = AutoModel.from_pretrained(model_name, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5vBezPyGG5u",
    "outputId": "6b976322-acfd-4b56-a629-a0f3c220407c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 4931, 1010, 2129, 2024, 2017, 2651, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the first example data to run the model and export it to ONNX\n",
    "\n",
    "sample = ['Hey, how are you today?']\n",
    "inputs = tokenizer(sample,\n",
    "                   padding=True,\n",
    "                   truncation=True,\n",
    "                   return_tensors=\"pt\"\n",
    "                   )\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T4LEH3EVFCx3",
    "outputId": "9fe5fe16-7061-4413-894a-1a64d8a42104"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Model exported at  ./onnx_models/all_MiniLM_L6-v2.onnx\n"
     ]
    }
   ],
   "source": [
    "## Convert Model to ONNX Format\n",
    "import os\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Set model to inference mode, which is required before exporting the model because some operators behave differently in\n",
    "# inference and training mode.\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "output_dir = os.path.join(\".\", \"onnx_models\")\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "export_model_path = os.path.join(output_dir, 'all_MiniLM_L6-v2.onnx')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "    torch.onnx.export(model,                                            # model being run\n",
    "                      args=tuple(inputs.values()),                      # model input (or a tuple for multiple inputs)\n",
    "                      f=export_model_path,                              # where to save the model (can be a file or file-like object)\n",
    "                      opset_version=11,                                 # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,                         # whether to execute constant folding for optimization\n",
    "                      input_names=['input_ids',                         # the model's input names\n",
    "                                    'attention_mask',\n",
    "                                    'token_type_ids'],\n",
    "                      output_names=['start', 'end'],                    # the model's output names\n",
    "                      dynamic_axes={'input_ids': symbolic_names,        # variable length axes\n",
    "                                    'attention_mask' : symbolic_names,\n",
    "                                    'token_type_ids' : symbolic_names,\n",
    "                                    'start' : symbolic_names,\n",
    "                                    'end' : symbolic_names})\n",
    "    print(\"Model exported at \", export_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xy8xeKmeGcH9",
    "outputId": "8883386b-5186-4cb8-d360-151e74b41a4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/encoder/layer.0/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layer.0/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layer.1/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layer.1/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layer.2/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layer.2/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layer.3/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layer.3/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layer.4/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layer.4/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layer.5/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layer.5/attention/self/MatMul_1]\n",
      "quantized model saved to:./onnx_models/all_MiniLM_L6-v2_quantised.onnx\n"
     ]
    }
   ],
   "source": [
    "# Generate Quantised ONNX model\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "quantized_model_path = \"./onnx_models/all_MiniLM_L6-v2_quantised.onnx\"\n",
    "\n",
    "onnx_opt_model = onnx.load(export_model_path)\n",
    "quantize_dynamic(export_model_path,\n",
    "                 quantized_model_path,\n",
    "                 weight_type=QuantType.QInt8)\n",
    "\n",
    "print(f\"quantized model saved to:{quantized_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoVURV2tHOOa"
   },
   "source": [
    "## Using Sentence Transformer Library (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wr7Gkt7IHNh6",
    "outputId": "eada33ac-fa4b-4044-a640-baee031067a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 34.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sentence Transformer SBERT cpu Inference time = 27.50 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_sbert_cpu = SentenceTransformer(model_name, device=torch.device(\"cpu\"))\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "total_samples = 1000\n",
    "\n",
    "latency = []\n",
    "\n",
    "for i in tqdm(range(total_samples)):\n",
    "    # data = dataset[i]\n",
    "    # inputs = {\n",
    "    #     'input_ids':      data[0].to(device).reshape(1, max_seq_length),\n",
    "    #     'attention_mask': data[1].to(device).reshape(1, max_seq_length),\n",
    "    #     'token_type_ids': data[2].to(device).reshape(1, max_seq_length)\n",
    "    # }\n",
    "    start = time.time()\n",
    "    embeddings_cpu = model_sbert_cpu.encode(sample)\n",
    "\n",
    "    latency.append(time.time() - start)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Sentence Transformer SBERT {} Inference time = {} ms\".format(device.type, format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Xlj2pIUBHNTv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNWKTp2_JS6O"
   },
   "source": [
    "## Vanila SBERT CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72DTEcbAIVMs",
    "outputId": "63da64dc-53bb-42ad-c3d2-f4f8464114ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:14<00:00, 69.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PyTorch cpu Inference time = 14.14 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "total_samples = 1000\n",
    "\n",
    "latency = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(total_samples)):\n",
    "        # data = dataset[i]\n",
    "        # inputs = {\n",
    "        #     'input_ids':      data[0].to(device).reshape(1, max_seq_length),\n",
    "        #     'attention_mask': data[1].to(device).reshape(1, max_seq_length),\n",
    "        #     'token_type_ids': data[2].to(device).reshape(1, max_seq_length)\n",
    "        # }\n",
    "        start = time.time()\n",
    "        outputs_cpu = mean_pooling(model(**inputs), inputs['attention_mask']).cpu().detach().numpy()\n",
    "\n",
    "        latency.append(time.time() - start)\n",
    "print(\"\\n\")\n",
    "print(\"PyTorch {} Inference time = {} ms\".format(device.type, format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOchbYq5J5m8"
   },
   "source": [
    "## ONNX Converted Model CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_TA9gC1G_0G"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DNPws-2II8Pi",
    "outputId": "d6cd2a03-a1ba-4dd4-8ce5-f69622751f8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 174.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "OnnxRuntime cpu Inference time = 5.63 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy\n",
    "\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "\n",
    "session = onnxruntime.InferenceSession(export_model_path, sess_options, providers=['CPUExecutionProvider'])\n",
    "\n",
    "latency = []\n",
    "for i in tqdm(range(total_samples)):\n",
    "    # data = dataset[i]\n",
    "    ort_inputs = {k:v.cpu().numpy() for k, v in inputs.items()}\n",
    "    # ort_inputs = {\n",
    "    #     'input_ids':  data[0].cpu().reshape(1, max_seq_length).numpy(),\n",
    "    #     'input_mask': data[1].cpu().reshape(1, max_seq_length).numpy(),\n",
    "    #     'segment_ids': data[2].cpu().reshape(1, max_seq_length).numpy()\n",
    "    # }\n",
    "    start = time.time()\n",
    "    op = session.run(None, ort_inputs)\n",
    "    op = torch.from_numpy(op[0])\n",
    "    ort_outputs_cpu = mean_pooling([op], inputs['attention_mask']).cpu().detach().numpy()\n",
    "    latency.append(time.time() - start)\n",
    "print(\"\\n\")\n",
    "print(\"OnnxRuntime cpu Inference time = {} ms\".format(format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eQz-K46ELwjO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M8SxicVKKo2y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ6wP96hLGLg"
   },
   "source": [
    "## Quantised ONNX Converted Model CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PmfScoFI8SY",
    "outputId": "6e8aca77-fe76-4266-d512-b024cbdfc9f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 271.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "OnnxRuntime Quantised cpu Inference time = 3.58 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy\n",
    "\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "\n",
    "session = onnxruntime.InferenceSession(quantized_model_path, sess_options, providers=['CPUExecutionProvider'])\n",
    "\n",
    "latency = []\n",
    "for i in tqdm(range(total_samples)):\n",
    "    # data = dataset[i]\n",
    "    ort_inputs = {k:v.cpu().numpy() for k, v in inputs.items()}\n",
    "    # ort_inputs = {\n",
    "    #     'input_ids':  data[0].cpu().reshape(1, max_seq_length).numpy(),\n",
    "    #     'input_mask': data[1].cpu().reshape(1, max_seq_length).numpy(),\n",
    "    #     'segment_ids': data[2].cpu().reshape(1, max_seq_length).numpy()\n",
    "    # }\n",
    "    start = time.time()\n",
    "    op = session.run(None, ort_inputs)\n",
    "    op = torch.from_numpy(op[0])\n",
    "    ort_outputs_quantised_cpu = mean_pooling([op], inputs['attention_mask']).cpu().detach().numpy()\n",
    "    latency.append(time.time() - start)\n",
    "print(\"\\n\")\n",
    "print(\"OnnxRuntime Quantised cpu Inference time = {} ms\".format(format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmCRz0c2hHjD",
    "outputId": "cd3ed995-d92a-45d6-9395-7330712f2bb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03050258,  0.0481415 ,  0.10223   ,  0.0486282 ,  0.01371921,\n",
       "        -0.05840193,  0.10347056, -0.03206377, -0.10671089, -0.00481879]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_cpu[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r9K7TVoZI8Wj",
    "outputId": "8b001b4d-88e0-4e18-a05b-9e288c077664"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03050258,  0.0481415 ,  0.10223   ,  0.0486282 ,  0.01371921,\n",
       "        -0.05840193,  0.10347056, -0.03206377, -0.10671089, -0.00481879]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_cpu[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IA60vhukI8ZR",
    "outputId": "fe0d2dd8-abe0-438c-e4a6-e58517b636eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03050255,  0.04814148,  0.10222996,  0.04862824,  0.01371919,\n",
       "        -0.05840201,  0.10347056, -0.03206379, -0.10671096, -0.00481878]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_outputs_cpu[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7ZX3WpoI8b3",
    "outputId": "c74f0dc0-6a2b-4123-d38d-3feb5dea40cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03625318,  0.02991685,  0.11215121,  0.03071814,  0.0087405 ,\n",
       "        -0.05622218,  0.09673943, -0.01424983, -0.11920619, -0.0072534 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_outputs_quantised_cpu[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nBxrkP2okm40"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jB_0boDPfSep"
   },
   "source": [
    "## Using Sentence Transformers library (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P502LBbzfX2u",
    "outputId": "e18c3a19-6de3-468b-b9e4-1513d4a738c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 117.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sentence Transformer SBERT cuda Inference time = 8.40 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_sbert_gpu = SentenceTransformer(model_name, device=device)\n",
    "total_samples = 1000\n",
    "\n",
    "latency = []\n",
    "\n",
    "for i in tqdm(range(total_samples)):\n",
    "    # data = dataset[i]\n",
    "    # inputs = {\n",
    "    #     'input_ids':      data[0].to(device).reshape(1, max_seq_length),\n",
    "    #     'attention_mask': data[1].to(device).reshape(1, max_seq_length),\n",
    "    #     'token_type_ids': data[2].to(device).reshape(1, max_seq_length)\n",
    "    # }\n",
    "    start = time.time()\n",
    "    embeddings_gpu = model_sbert_gpu.encode(sample)\n",
    "\n",
    "    latency.append(time.time() - start)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Sentence Transformer SBERT {} Inference time = {} ms\".format(device.type, format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3DvKzdqQfX6Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uxmc5hzYL2NC"
   },
   "source": [
    "## Vanila SBERT GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3GstfifLv8V",
    "outputId": "d1f703c9-6c08-4d18-e773-00cfec2aeabc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 180.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PyTorch cuda Inference time = 5.47 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Set model to inference mode, which is required before exporting the model because some operators behave differently in\n",
    "# inference and training mode.\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "sample = ['Hey, how are you today?']\n",
    "inputs = tokenizer(sample,\n",
    "                   padding=True,\n",
    "                   truncation=True,\n",
    "                   return_tensors=\"pt\"\n",
    "                   )\n",
    "inputs.to(device)\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "total_samples = 1000\n",
    "\n",
    "latency = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(total_samples)):\n",
    "        # data = dataset[i]\n",
    "        # inputs = {\n",
    "        #     'input_ids':      data[0].to(device).reshape(1, max_seq_length),\n",
    "        #     'attention_mask': data[1].to(device).reshape(1, max_seq_length),\n",
    "        #     'token_type_ids': data[2].to(device).reshape(1, max_seq_length)\n",
    "        # }\n",
    "        start = time.time()\n",
    "        outputs_gpu = mean_pooling(model(**inputs), inputs['attention_mask']).cpu().detach().numpy()\n",
    "        latency.append(time.time() - start)\n",
    "print(\"\\n\")\n",
    "print(\"PyTorch {} Inference time = {} ms\".format(device.type, format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5E33fFGMetA"
   },
   "source": [
    "## Onnx Converted Model GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjrlRjT7L4uJ",
    "outputId": "38a906c8-1334-4b02-c325-afd3da61e616"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 439.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "OnnxRuntime GPU Inference time = 2.09 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy\n",
    "\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "\n",
    "session = onnxruntime.InferenceSession(export_model_path, sess_options, providers=['CUDAExecutionProvider'])\n",
    "\n",
    "latency = []\n",
    "for i in tqdm(range(total_samples)):\n",
    "    # data = dataset[i]\n",
    "    ort_inputs = {k:v.cpu().numpy() for k, v in inputs.items()}\n",
    "    # ort_inputs = {\n",
    "    #     'input_ids':  data[0].cpu().reshape(1, max_seq_length).numpy(),\n",
    "    #     'input_mask': data[1].cpu().reshape(1, max_seq_length).numpy(),\n",
    "    #     'segment_ids': data[2].cpu().reshape(1, max_seq_length).numpy()\n",
    "    # }\n",
    "    start = time.time()\n",
    "    op = session.run(None, ort_inputs)\n",
    "    op = torch.from_numpy(op[0])\n",
    "    ort_outputs_gpu = mean_pooling([op], inputs['attention_mask'].cpu()).cpu().detach().numpy()\n",
    "    latency.append(time.time() - start)\n",
    "print(\"\\n\")\n",
    "print(\"OnnxRuntime GPU Inference time = {} ms\".format(format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cle776NDND50"
   },
   "source": [
    "## Quantised ONNX Converted Model GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Y0FYUqYNC-C",
    "outputId": "5ee0b311-95ec-454a-aeb0-9842ebd5f6da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 172.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "OnnxRuntime Quantised GPU Inference time = 5.55 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy\n",
    "\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "\n",
    "session = onnxruntime.InferenceSession(quantized_model_path, sess_options, providers=['CUDAExecutionProvider'])\n",
    "\n",
    "latency = []\n",
    "for i in tqdm(range(total_samples)):\n",
    "    # data = dataset[i]\n",
    "    ort_inputs = {k:v.cpu().numpy() for k, v in inputs.items()}\n",
    "    # ort_inputs = {\n",
    "    #     'input_ids':  data[0].cpu().reshape(1, max_seq_length).numpy(),\n",
    "    #     'input_mask': data[1].cpu().reshape(1, max_seq_length).numpy(),\n",
    "    #     'segment_ids': data[2].cpu().reshape(1, max_seq_length).numpy()\n",
    "    # }\n",
    "    start = time.time()\n",
    "    op = session.run(None, ort_inputs)\n",
    "    op = torch.from_numpy(op[0])\n",
    "    ort_outputs_quantised_gpu = mean_pooling([op], inputs['attention_mask'].cpu()).cpu().detach().numpy()\n",
    "    latency.append(time.time() - start)\n",
    "print(\"\\n\")\n",
    "print(\"OnnxRuntime Quantised GPU Inference time = {} ms\".format(format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mzL9h8BhhN-l",
    "outputId": "5dbb10fe-f00d-4d1a-8c53-b859e3f1868d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03050257,  0.04814149,  0.10222998,  0.04862824,  0.01371918,\n",
       "        -0.05840195,  0.10347056, -0.03206379, -0.10671094, -0.00481877]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_gpu[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RxavGGy0L4wv",
    "outputId": "e58dfc09-098b-4e63-c2ea-561fb7069d9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03050257,  0.04814149,  0.10222998,  0.04862824,  0.01371918,\n",
       "        -0.05840195,  0.10347056, -0.03206379, -0.10671094, -0.00481877]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_gpu[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJoZoxjAL40D",
    "outputId": "8b165b0b-dd82-4414-e270-71efd9dfc1a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03050256,  0.04814154,  0.10222998,  0.04862824,  0.01371915,\n",
       "        -0.05840195,  0.10347054, -0.0320638 , -0.10671094, -0.00481879]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_outputs_gpu[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wxXLZ4-Npi7",
    "outputId": "af70710b-1d01-400a-9b91-1ebaf8f672ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03625319,  0.02991689,  0.1121512 ,  0.03071815,  0.0087405 ,\n",
       "        -0.05622218,  0.09673939, -0.01424985, -0.11920615, -0.0072534 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_outputs_quantised_gpu[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1Ni2WabmYhzo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
